{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "    Filename: training-on-google-colab.ipynb\n",
        "    Usage: Fine-tune the base model using the SQuAD2 dataset\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ-WRhacmCcG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "def mount():\n",
        "  drive.mount('/content/drive')\n",
        "  os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Extract-QA\")\n",
        "\n",
        "mount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brF8cJ1Bly29"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install textbrewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLF-5NDmAiW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1zJBzlA50PT"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "pretrain_model = \"google/bert_uncased_L-12_H-768_A-12\"\n",
        "\n",
        "device = \"cuda\"\n",
        "epochs = 4\n",
        "batch_size = 24\n",
        "learning_rate = 3e-5\n",
        "\n",
        "max_seq_length = 384\n",
        "doc_stride = 128\n",
        "max_query_length = 256\n",
        "\n",
        "model_output_folder = \"outputs\"\n",
        "model_output_name = \"bert-base-squad2-uncased\"\n",
        "squad_data_folder = \"data/squad\"\n",
        "\n",
        "checkpoint_folder = \"checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKpTarAo9ICG"
      },
      "outputs": [],
      "source": [
        "# Reset environment\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "shutil.rmtree(model_output_folder, ignore_errors=True)\n",
        "Path(model_output_folder).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI0a70uKltnR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm_notebook\n",
        "from transformers import AdamW,BertConfig,BertForQuestionAnswering,BertTokenizer,get_linear_schedule_with_warmup,squad_convert_examples_to_features\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits,squad_evaluate\n",
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor\n",
        "\n",
        "# https://github.com/huggingface/transformers/issues/701\n",
        "# Google repo\n",
        "# batch sizes: 8, 16, 32, 64, 128\n",
        "# learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
        "model_name = pretrain_model\n",
        "\n",
        "processor = SquadV2Processor()\n",
        "examples = processor.get_train_examples(f\"{squad_data_folder}/\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "\n",
        "features,train_dataset = squad_convert_examples_to_features(\n",
        "    examples=examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=True,\n",
        "    return_dataset=\"pt\"\n",
        ")\n",
        "\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjGX99DEwtJK"
      },
      "outputs": [],
      "source": [
        "#Start training\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "t_total = len(train_dataloader) * epochs\n",
        "\n",
        "# Prepare optimizer and schedule \n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
        "\n",
        "print(\"  Num examples = \", len(train_dataset))\n",
        "print(\"  Total optimization steps = \", t_total)\n",
        "\n",
        "steps = 1\n",
        "tr_loss = 0.0\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch:{}'.format(epoch+1))\n",
        "    epoch_iterator = tqdm.notebook.tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "        model.train()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[0],\n",
        "            \"attention_mask\": batch[1],\n",
        "            \"token_type_ids\": batch[2],\n",
        "            \"start_positions\": batch[3],\n",
        "            \"end_positions\": batch[4],\n",
        "        }\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        iter_loss = loss.item()\n",
        "        tr_loss += iter_loss\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()  \n",
        "        model.zero_grad()\n",
        "        steps += 1\n",
        "        \n",
        "        # Log \n",
        "        if steps % 100 == 0:\n",
        "            print('steps = {}, loss = {}, avg loss = {}'.format(steps,iter_loss,tr_loss/steps))\n",
        "    mount()\n",
        "    tokenizer.save_pretrained(f\"{checkpoint_folder}/{model_output_name}-epoch-{epoch}\")\n",
        "    model.save_pretrained(f\"{checkpoint_folder}/{model_output_name}-epoch-{epoch}\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': tr_loss\n",
        "        }, f\"{checkpoint_folder}/{model_output_name}-epoch-{epoch}/state-snapshot\")\n",
        "\n",
        "mount()\n",
        "tokenizer.save_pretrained(f\"{model_output_folder}/{model_output_name}\")\n",
        "model.save_pretrained(f\"{model_output_folder}/{model_output_name}\")\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'loss': tr_loss\n",
        "    }, f\"{model_output_folder}/{model_output_name}/state-snapshot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoEuKkPM5HsC"
      },
      "outputs": [],
      "source": [
        "# Discount runtime automatically\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ixq9tkNMw8FU"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
