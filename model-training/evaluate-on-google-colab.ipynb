{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Por6fMvmNc5f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Extract-QA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "n_oGBWkUOGGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "tmodel_name = \"zelcakok/bert-base-squad2-uncased\"\n",
        "AutoModelForQuestionAnswering.from_pretrained(tmodel_name).save_pretrained(f\"checkpoints/{tmodel_name}\")\n",
        "AutoTokenizer.from_pretrained(tmodel_name).save_pretrained(f\"checkpoints/{tmodel_name}\")\n"
      ],
      "metadata": {
        "id": "3hl_Dq91Lhv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "device = \"cuda\"\n",
        "batch_size = 24\n",
        "\n",
        "max_seq_length = 384\n",
        "doc_stride = 128\n",
        "max_query_length = 256\n",
        "\n",
        "model_output_folder = \"checkpoints\"\n",
        "model_output_name = \"zelcakok-bert-base-squad2-uncased\"\n",
        "squad_data_folder = \"data/squad\""
      ],
      "metadata": {
        "id": "EbmSMSL0L_ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor\n",
        "from transformers import BertTokenizer, squad_convert_examples_to_features\n",
        "from json import dumps\n",
        "import os\n",
        "\n",
        "model_folder = model_output_folder\n",
        "model_name = model_output_name\n",
        "\n",
        "processor = SquadV2Processor()\n",
        "examples = processor.get_dev_examples('data/squad/')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(f\"{model_folder}/{model_name}\")\n",
        "\n",
        "features,eval_dataset = squad_convert_examples_to_features(\n",
        "    examples=examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=False,\n",
        "    return_dataset=\"pt\"\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, SequentialSampler\n",
        "from typing import Any, BinaryIO, Dict, List, Optional, Tuple, Union\n",
        "from transformers import BertForQuestionAnswering, AutoModel\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits,squad_evaluate\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "test_model = BertForQuestionAnswering.from_pretrained(f\"{model_folder}/{model_name}\")\n",
        "test_model.to(device)\n",
        "\n",
        "eval_sampler = SequentialSampler(eval_dataset)\n",
        "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=batch_size)\n",
        "\n",
        "print(\"  Num examples = \", len(eval_dataset))\n",
        "\n",
        "all_results = []\n",
        "\n",
        "def to_tuple(self) -> Tuple[Any]:\n",
        "    \"\"\"\n",
        "    Convert self to a tuple containing all the attributes/keys that are not ``None``.\n",
        "    \"\"\"\n",
        "    return tuple(self[k] for k in self.keys())\n",
        "\n",
        "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "    test_model.eval()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[0],\n",
        "            \"attention_mask\": batch[1],\n",
        "            \"token_type_ids\": batch[2],\n",
        "        }\n",
        "\n",
        "        feature_indices = batch[3]\n",
        "\n",
        "        outputs = test_model(**inputs)\n",
        "\n",
        "    for i, feature_index in enumerate(feature_indices):\n",
        "        eval_feature = features[feature_index.item()]\n",
        "        unique_id = int(eval_feature.unique_id)\n",
        "        output = [(output[i]).detach().cpu().tolist() for output in outputs.to_tuple()]\n",
        "        start_logits, end_logits = output\n",
        "        result = SquadResult(unique_id, start_logits, end_logits)\n",
        "        all_results.append(result)\n",
        "\n",
        "output_folder = f\"evaluate-result/{model_name}\"\n",
        "shutil.rmtree(output_folder, ignore_errors=True)\n",
        "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "predictions = compute_predictions_logits(\n",
        "    examples,\n",
        "    features,\n",
        "    all_results,\n",
        "    n_best_size=20,\n",
        "    max_answer_length=30,\n",
        "    do_lower_case=True,\n",
        "    output_prediction_file=f\"{output_folder}/predictions.json\",\n",
        "    output_nbest_file=f\"{output_folder}/nbest_predictions.json\",\n",
        "    output_null_log_odds_file=f\"{output_folder}/null_odds_predictions.json\",\n",
        "    verbose_logging=False,\n",
        "    version_2_with_negative=True,\n",
        "    null_score_diff_threshold=0.0,\n",
        "    tokenizer=tokenizer,)\n",
        "\n",
        "# Compute the F1 and exact scores.\n",
        "results = squad_evaluate(examples, predictions)\n",
        "\n",
        "with open(f\"{output_folder}/result.json\", \"w\") as f:\n",
        "  f.write(dumps(results, indent=4))"
      ],
      "metadata": {
        "id": "WgxoXQt4Nk_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discount runtime automatically\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "lZZT6NLcFXJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}